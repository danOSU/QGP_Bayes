{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Bayesian parameter inference of a **Cascading Hybrid Relativistic Heavy Ion Collision Model**.\n",
    ">**1. Loading simulation data**\n",
    "    - This section show how to load simulation data from the disk. Anyone who wish to use the code to do another Bayesian analysis for relativistic heavy ion collision should do minor change to this section that suits their model and simulation data.\n",
    "            --Python modules used\n",
    "                - pandas\n",
    "    \n",
    ">**2. Preprocessing of the simulation data**\n",
    "    - We perform a scaling transform and then a Principal Component Analysis (PCA) to reduce the dimensions of the data. We keep only 10 PCs for the next steps of the analysis.\n",
    "            --Python modules used\n",
    "                - numpy, sklearn\n",
    "                \n",
    ">**3. Load priors and experimental data**\n",
    "    - We use uniform priors and load the bounds for the priors from a file saved on the disk. Relevant experimental data is also loaded from the disk. A new analysis with new data will have to make changes to this part of the code. \n",
    "            --Python modules used\n",
    "                - pandas\n",
    "\n",
    ">**4. Building Emulators**\n",
    "    - Since the simulations are computationally expensive we need surrogate models that can be trained on limited number of points and then used to predict the simulation output at any given point. For this purpose we use Gaussian Processes and they also give the uncertainty in the prediction which is very crucial for the reliability of the Bayesian parameter extraction method. \n",
    "            --Python modules used\n",
    "                - sklearn Gaussian Processors, numpy\n",
    "                \n",
    ">**5. Bayesian Parameter Estimation**\n",
    "    - This is where everything comes together and magic happens! The Bayes theorem is used to find the posterior of the model parameters using the experimental data.\n",
    "            --Python modules used\n",
    "                - pandas, numpy, seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name=\"JETSCAPE_bayes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import GPy\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from numpy.linalg import inv\n",
    "import sklearn, matplotlib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor as gpr\n",
    "from sklearn.gaussian_process import kernels as krnl\n",
    "import scipy.stats as st\n",
    "from scipy import optimize\n",
    "\n",
    "import emcee\n",
    "import ptemcee\n",
    "import h5py\n",
    "from scipy.linalg import lapack\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing import cpu_count\n",
    "import time\n",
    "sns.set(\"notebook\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading simulation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "modulenames = set(sys.modules) & set(globals())\n",
    "allmodules = [sys.modules[name] for name in modulenames]\n",
    "allmodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saved emulator name\n",
    "EMU='PbPb2760_emulators_scikit.dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the figures and data files\n",
    "PROJECT_ROOT_DIR = \"Results\"\n",
    "FIGURE_ID = \"Results/FigureFiles\"\n",
    "DATA_ID = \"DataFiles/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(PROJECT_ROOT_DIR):\n",
    "    os.mkdir(PROJECT_ROOT_DIR)\n",
    "\n",
    "if not os.path.exists(FIGURE_ID):\n",
    "    os.makedirs(FIGURE_ID)\n",
    "\n",
    "if not os.path.exists(DATA_ID):\n",
    "    os.makedirs(DATA_ID)\n",
    "\n",
    "def image_path(fig_id):\n",
    "    return os.path.join(FIGURE_ID, fig_id)\n",
    "\n",
    "def data_path(dat_id):\n",
    "    return os.path.join(DATA_ID, dat_id)\n",
    "\n",
    "def save_fig(fig_id):\n",
    "    plt.savefig(image_path(fig_id) + \".png\", format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design points\n",
    "design = pd.read_csv(filepath_or_buffer=\"DataFiles/PbPb2760_design\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#design.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulation outputs at the design points\n",
    "simulation = pd.read_csv(filepath_or_buffer=\"DataFiles/PbPb2760_simulation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = design.values\n",
    "Y = simulation.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"X.shape : \"+ str(X.shape) )\n",
    "print( \"Y.shape : \"+ str(Y.shape) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model parameter names in Latex compatble form\n",
    "model_param_dsgn = ['$N$[$2.76$TeV]',\n",
    " '$p$',\n",
    " '$\\\\sigma_k$',\n",
    " '$w$ [fm]',\n",
    " '$d_{\\\\mathrm{min}}$ [fm]',\n",
    " '$\\\\tau_R$ [fm/$c$]',\n",
    " '$\\\\alpha$',\n",
    " '$T_{\\\\eta,\\\\mathrm{kink}}$ [GeV]',\n",
    " '$a_{\\\\eta,\\\\mathrm{low}}$ [GeV${}^{-1}$]',\n",
    " '$a_{\\\\eta,\\\\mathrm{high}}$ [GeV${}^{-1}$]',\n",
    " '$(\\\\eta/s)_{\\\\mathrm{kink}}$',\n",
    " '$(\\\\zeta/s)_{\\\\max}$',\n",
    " '$T_{\\\\zeta,c}$ [GeV]',\n",
    " '$w_{\\\\zeta}$ [GeV]',\n",
    " '$\\\\lambda_{\\\\zeta}$',\n",
    " '$b_{\\\\pi}$',\n",
    " '$T_{\\\\mathrm{sw}}$ [GeV]']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocessing of simulation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling the data to be zero mean and unit variance for each feature\n",
    "SS  =  StandardScaler(copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of u (485, 485) shape of s (110,) shape of vh (110, 110)\n"
     ]
    }
   ],
   "source": [
    "#Singular Value decomposition\n",
    "u, s, vh = np.linalg.svd(SS.fit_transform(Y), full_matrices=True)\n",
    "print(f'shape of u {u.shape} shape of s {s.shape} shape of vh {vh.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whiten and project data to principal component axis (only keeping first 10 PCs)\n",
    "pc_tf_data=u[:,0:10] * math.sqrt(u.shape[0]-1)\n",
    "#Scale Transformation from PC space to original data space\n",
    "inverse_tf_matrix= np.diag(s[0:10]) @ vh[0:10,:] * SS.scale_.reshape(1,110)/ math.sqrt(u.shape[0]-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A check for consistency of our explicit PCA calculation from scikit-learn library PCA module.\n",
    "We need access to the scaling transform involved in PCA for uncertainty propagation of emulator predictions. And also it is needed to find the truncation error of keeping only the dominant PC of data. So we prefer to rather use explicit PCA using SVD rather than using the scikit-learn PCA module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check the PC transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed data shape (485, 10)\n"
     ]
    }
   ],
   "source": [
    "# White actually multiply each PC by sqrt of n_samples-1\n",
    "pca_analysis = PCA(n_components=10, whiten=True, svd_solver='auto')\n",
    "pca_analysis.fit(SS.fit_transform(Y))\n",
    "scikit_pc_tf_data=pca_analysis.transform(SS.fit_transform(Y))\n",
    "print(f'Transformed data shape {scikit_pc_tf_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print True if the transformed data is same from SVD aproach and scikitlearn PCA methods\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#The sign difference is OK. That is why we only compare abs value of transformed Data\n",
    "#explanation https://stackoverflow.com/questions/44765682/in-sklearn-decomposition-pca-why-are-components-negative\n",
    "print(f'Print True if the transformed data is same from SVD aproach and scikit\\\n",
    "learn PCA methods\\n{np.allclose(np.abs(scikit_pc_tf_data),np.abs(pc_tf_data))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check inverse PC transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print True if the inverse transformed data is same from SVD aproach and scikitlearn PCA methods\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "scikit_inverse_tf_data=SS.inverse_transform(pca_analysis.inverse_transform(scikit_pc_tf_data))\n",
    "inverse_tf_data=pc_tf_data @ inverse_tf_matrix + np.repeat([SS.mean_],repeats=485, axis=0)\n",
    "print(f'Print True if the inverse transformed data is same from SVD aproach and scikit\\\n",
    "learn PCA methods\\n{np.allclose(scikit_inverse_tf_data,inverse_tf_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load priors and experimental data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounds for parametrs in the emulator are same as prior ranges so\n",
    "prior_df = pd.read_csv(filepath_or_buffer=\"DataFiles/PbPb2760_prior\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prior_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the experiment observables (110,) and shape of the experimental error variance(110,)\n"
     ]
    }
   ],
   "source": [
    "experiment=pd.read_csv(filepath_or_buffer=\"DataFiles/PbPb2760_experiment\",index_col=0)\n",
    "experiment.head()\n",
    "y_exp=experiment.loc['mean'].values\n",
    "y_exp_variance=experiment.loc['variance'].values\n",
    "print(f'Shape of the experiment observables {y_exp.shape} and shape of the experimental error variance{y_exp_variance.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Building emulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_max=prior_df.loc['max'].values\n",
    "design_min=prior_df.loc['min'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved emulators exists and overide is prohibited\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "design=X\n",
    "overide=False\n",
    "input_dim=len(design_max)\n",
    "ptp = design_max - design_min\n",
    "bound=zip(design_min,design_max)\n",
    "if (os.path.exists(data_path(EMU))) and (overide==False):\n",
    "    print('Saved emulators exists and overide is prohibited')\n",
    "    with open(data_path(EMU),\"rb\") as f:\n",
    "        Emulators=pickle.load(f)\n",
    "else:\n",
    "    Emulators=[]\n",
    "    for i in range(0,10):\n",
    "        start_time = time.time()\n",
    "        kernel=1*krnl.RBF(length_scale=ptp,length_scale_bounds=np.outer(ptp, (4e-1, 1e2)))+ krnl.WhiteKernel(noise_level=.1, noise_level_bounds=(1e-2, 1e2))#+krnl.ConstantKernel()\n",
    "        GPR=gpr(kernel=kernel,n_restarts_optimizer=4,alpha=0.0000000001)\n",
    "        GPR.fit(design,pc_tf_data[:,i].reshape(-1,1))\n",
    "        print(f'GPR score is {GPR.score(design,pc_tf_data[:,i])} \\n')\n",
    "        #print(f'GPR log_marginal likelihood {GPR.log_marginal_likelihood()} \\n')\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        Emulators.append(GPR)\n",
    "\n",
    "if (overide==True) or not(os.path.exists(data_path(EMU))):\n",
    "    with open(data_path(EMU),\"wb\") as f:\n",
    "        pickle.dump(Emulators,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_observables(model_parameters):\n",
    "    \"\"\"Predicts the observables for any model parameter value using the trained emulators.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Theta_input : Model parameter values. Should be an 1D array of 17 model parametrs.\n",
    "    Return\n",
    "    ------\n",
    "    Mean value and full error covaraiance matrix of the prediction is returened. \"\"\"\n",
    "    \n",
    "    mean=[]\n",
    "    variance=[]\n",
    "    theta=np.array(model_parameters).flatten()\n",
    "    \n",
    "    if len(theta)!=17:\n",
    "        raise TypeError('The input model_parameters array does not have the right dimensions')\n",
    "    else: \n",
    "        theta=np.array(theta).reshape(1,17)\n",
    "        for i in range(0,10):\n",
    "            mn,std=Emulators[i].predict(theta,return_std=True)\n",
    "            mean.append(mn)\n",
    "            variance.append(std**2)\n",
    "    mean=np.array(mean).reshape(1,-1)\n",
    "    inverse_transformed_mean=mean @ inverse_tf_matrix + np.array(SS.mean_).reshape(1,-1)\n",
    "    variance_matrix=np.diag(np.array(variance).flatten())\n",
    "    A_p=np.abs(inverse_tf_matrix)\n",
    "    inverse_transformed_variance=np.einsum('ik,kl,lj-> ij', A_p.T, variance_matrix, A_p, optimize=False)\n",
    "    return inverse_transformed_mean, inverse_transformed_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAP_params = {'Pb-Pb-2760': [14.128, 0.089, 1.054, 1.064, 4.227, 1.507, 0.113, 0.223, -1.585, 0.32, 0.056, 0.11, 0.16, 0.093, -0.084, 4.666, 0.136],\n",
    "#              'Au-Au-200' : [5.821, 0.089, 1.054, 1.064, 4.227, 1.507, 0.113, 0.223, -1.585, 0.32, 0.056, 0.11, 0.16, 0.093, -0.084, 4.666, 0.136]\n",
    "#                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mm,vv=predict_observables(MAP_params['Pb-Pb-2760'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Bayesian parameter inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prior(model_parameters):\n",
    "    \"\"\"Evaluvate the prior at model prameter values. \n",
    "    If all parameters are inside bounds function will return 0 otherwise -inf\"\"\"\n",
    "    X = np.array(model_parameters).reshape(1,-1)\n",
    "    lower = np.all(X >= design_min)\n",
    "    upper = np.all(X <= design_max)\n",
    "    if (lower and upper):\n",
    "        lp=0\n",
    "    # lp = np.log(st.beta.pdf(X,5,1,dsgn_min_ut.reshape(1,-1),(dsgn_max_ut-dsgn_min_ut).reshape(1,-1))).sum()\n",
    "    else:\n",
    "        lp = -np.inf\n",
    "    return lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mvn_loglike(y, cov):\n",
    "    \"\"\"\n",
    "    Evaluate the multivariate-normal log-likelihood for difference vector `y`\n",
    "    and covariance matrix `cov`:\n",
    "\n",
    "        log_p = -1/2*[(y^T).(C^-1).y + log(det(C))] + const.\n",
    "\n",
    "    The likelihood is NOT NORMALIZED, since this does not affect MCMC.  The\n",
    "    normalization const = -n/2*log(2*pi), where n is the dimensionality.\n",
    "\n",
    "    Arguments `y` and `cov` MUST be np.arrays with dtype == float64 and shapes\n",
    "    (n) and (n, n), respectively.  These requirements are NOT CHECKED.\n",
    "\n",
    "    The calculation follows algorithm 2.1 in Rasmussen and Williams (Gaussian\n",
    "    Processes for Machine Learning).\n",
    "\n",
    "    \"\"\"\n",
    "    # Compute the Cholesky decomposition of the covariance.\n",
    "    # Use bare LAPACK function to avoid scipy.linalg wrapper overhead.\n",
    "    L, info = lapack.dpotrf(cov, clean=False)\n",
    "\n",
    "    if info < 0:\n",
    "        raise ValueError(\n",
    "            'lapack dpotrf error: '\n",
    "            'the {}-th argument had an illegal value'.format(-info)\n",
    "        )\n",
    "    elif info < 0:\n",
    "        raise np.linalg.LinAlgError(\n",
    "            'lapack dpotrf error: '\n",
    "            'the leading minor of order {} is not positive definite'\n",
    "            .format(info)\n",
    "        )\n",
    "\n",
    "    # Solve for alpha = cov^-1.y using the Cholesky decomp.\n",
    "    alpha, info = lapack.dpotrs(L, y)\n",
    "\n",
    "    if info != 0:\n",
    "        raise ValueError(\n",
    "            'lapack dpotrs error: '\n",
    "            'the {}-th argument had an illegal value'.format(-info)\n",
    "        )\n",
    "  #  print(L.diagonal())\n",
    "    a=np.ones(len(L.diagonal()))*1e-10\n",
    "    #print(a)\n",
    "    #print(L)\n",
    "   # L=L+np.diag(a)\n",
    "    if np.all(L.diagonal()>0):\n",
    "        return -.5*np.dot(y, alpha) - np.log(L.diagonal()).sum()\n",
    "    else:\n",
    "        print(L.diagonal())\n",
    "        raise ValueError(\n",
    "            'L has negative values on diagonal {}'.format(L.diagonal())\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariance truncation error from PC is not yet included\n",
    "def log_posterior(model_parameters):\n",
    "    mn,var=predict_observables(model_parameters)\n",
    "    delta_y=mn-y_exp.reshape(1,-1)\n",
    "    delta_y=delta_y.flatten()\n",
    "    \n",
    "    exp_var=np.diag(y_exp_variance)\n",
    "    \n",
    "    total_var=var + exp_var\n",
    "    #only_diagonal=np.diag(total_var.diagonal())\n",
    "    return log_prior(model_parameters) + mvn_loglike(delta_y,total_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log_posterior(MAP_params['Pb-Pb-2760'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18.34198215 -0.51093231  0.3217179  ...  0.65346213  7.83546353\n",
      "   0.1391635 ]\n",
      " [15.59632931 -0.29613632  1.61645809 ... -0.47397429  3.64720863\n",
      "   0.13628302]\n",
      " [10.09353179 -0.1309434   0.53615213 ... -0.52552993  7.11738855\n",
      "   0.15538712]\n",
      " ...\n",
      " [11.5608728  -0.35879809  0.46414146 ...  0.36004681  5.87533607\n",
      "   0.15734693]\n",
      " [15.78970774  0.56817962  1.19008324 ... -0.28289377  7.90743215\n",
      "   0.14348173]\n",
      " [14.55231788  0.49692012  1.44124871 ...  0.76581393  7.03675562\n",
      "   0.15190302]]\n",
      "MCMC sampling using emcee (affine-invariant ensamble sampler) with 200 walkers\n",
      "burn in sampling started\n",
      "burn was true going to burnin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [06:00<00:00,  1.39it/s]\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean acceptance fraction: 0.209 (in total 100000 steps)\n",
      "Burn in completed. Now running the samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [11:51<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean acceptance fraction: 0.162 (in total 200000 steps)\n",
      "Time it took to generate MCMC chain 1080.0071601867676\n"
     ]
    }
   ],
   "source": [
    "\n",
    "do_mcmc = True\n",
    "st = time.time()\n",
    "Burn=True\n",
    "from multiprocessing import Pool\n",
    "ndim = 17  # number of parameters in the model\n",
    "nwalkers = 200  # number of MCMC walkers\n",
    "nburn = 500 # \"burn-in\" period to let chains stabilize\n",
    "nsteps =1000  # number of MCMC steps to take\n",
    "filename = data_path(name+\".h5\")\n",
    "\n",
    "if do_mcmc==True:\n",
    "    os.remove(filename)\n",
    "    backend = emcee.backends.HDFBackend(filename)\n",
    "    starting_guesses = design_min + (design_max - design_min) * np.random.rand(nwalkers,ndim)\n",
    "    print(starting_guesses)\n",
    "    print(\"MCMC sampling using emcee (affine-invariant ensamble sampler) with {0} walkers\".format(nwalkers))\n",
    "    with Pool() as pool:\n",
    "        sampler = emcee.EnsembleSampler(nwalkers, ndim, log_posterior, pool=pool, backend=backend)\n",
    "        print('burn in sampling started')\n",
    "        if Burn==True:\n",
    "            backend.reset(nwalkers, ndim)\n",
    "            print('burn was true going to burnin')\n",
    "            pos= sampler.run_mcmc(starting_guesses, nburn,progress=True,store = True)\n",
    "        print(\"Mean acceptance fraction: {0:.3f} (in total {1} steps)\".format(np.mean(sampler.acceptance_fraction),nwalkers*nburn))\n",
    "\n",
    "        print('Burn in completed. Now running the samples')\n",
    "        sampler.run_mcmc(initial_state=None,nsteps=nsteps,progress=True, tune= False)  \n",
    "        print(\"Mean acceptance fraction: {0:.3f} (in total {1} steps)\".format(np.mean(sampler.acceptance_fraction),nwalkers*nsteps))\n",
    "        \n",
    "        # discard burn-in points and flatten the walkers; the shape of samples is (nwalkers*nsteps, ndim)\n",
    "        samples = backend.get_chain(flat=True,discard=nburn)\n",
    "        samples_df=pd.DataFrame(samples, columns=model_param_dsgn)\n",
    "        samples_df.to_csv(name)\n",
    "else:\n",
    "    samples_df = pd.read_csv(name, index_col=0)\n",
    "    \n",
    "et = time.time()\n",
    "print(f'Time it took to generate MCMC chain {et-st}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_map_param = False\n",
    "if find_map_param == True:\n",
    "    bounds=[(a,b) for (a,b) in zip(design_min,design_max)]\n",
    "    rslt = optimize.differential_evolution(lambda x: -log_posterior(x), \n",
    "                                           bounds=bounds,\n",
    "                                          disp=True,\n",
    "                                          tol=1e-7,\n",
    "                                         )\n",
    "    print(rslt.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_values_saved = np.array([14.0139395 ,  0.01907986,  0.99684969,  1.00082965,  1.265258  ,\n",
    "        1.4427919 ,  0.06691838,  0.23605955, -0.12761594,  0.01850112,\n",
    "        0.12516493,  0.11635104,  0.18826292,  0.10610828,  0.08978906,\n",
    "        5.60047767,  0.13516286])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.divide(map_values_saved,MAP_params['Pb-Pb-2760'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.color_palette('bright')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "sns.set_style(\"ticks\")\n",
    "map_parameters=map_values_saved.flatten()\n",
    "sns.set_palette('bright')\n",
    "observables_to_plot=[0, 1, 2 ,3 , 4, 5, 6, 15, 16]\n",
    "g = sns.PairGrid(samples_df.iloc[:,observables_to_plot], corner=True, diag_sharey=False)\n",
    "g.map_lower(sns.histplot, bins=40, color=sns.color_palette()[4])\n",
    "#g.map_upper(sns.kdeplot, shade=True, color=sns.color_palette()[0])\n",
    "g.map_diag(sns.kdeplot, linewidth=2, shade=True, color=sns.color_palette()[9])\n",
    "for n,i in enumerate(observables_to_plot):\n",
    "    ax=g.axes[n][n]\n",
    "    ax.axvline(x=map_parameters[i], ls='--', c=sns.color_palette()[9])\n",
    "    ax.text(0,0.9,s= f'{map_parameters[i]:.3f}', transform=ax.transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "save_fig(name+\"partial\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "sns.set_style(\"ticks\")\n",
    "map_parameters=map_values_saved.flatten()\n",
    "sns.set_palette('bright')\n",
    "observables_to_plot=[7, 8 , 9, 10, 11, 12, 13, 14]\n",
    "g = sns.PairGrid(samples_df.iloc[:,observables_to_plot], corner=True, diag_sharey=False)\n",
    "g.map_lower(sns.histplot, bins=10, color=sns.color_palette()[5])\n",
    "g.map_diag(sns.kdeplot, linewidth=2, shade=True, color=sns.color_palette()[1])\n",
    "for n,i in enumerate(observables_to_plot):\n",
    "    ax=g.axes[n][n]\n",
    "    ax.axvline(x=map_parameters[i], ls='--', c=sns.color_palette()[1])\n",
    "    ax.text(0,0.9,s= f'{map_parameters[i]:.3f}', transform=ax.transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "save_fig(name+\"viscous\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define temperature dependent viscosity functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeta_over_s(T, zmax, T0, width, asym):\n",
    "    DeltaT = T - T0\n",
    "    sign = 1 if DeltaT>0 else -1\n",
    "    x = DeltaT/(width*(1.+asym*sign))\n",
    "    return zmax/(1.+x**2)\n",
    "zeta_over_s = np.vectorize(zeta_over_s)\n",
    "\n",
    "def eta_over_s(T, T_k, alow, ahigh, etas_k):\n",
    "    if T < T_k:\n",
    "        y = etas_k + alow*(T-T_k)\n",
    "    else:\n",
    "        y = etas_k + ahigh*(T-T_k)\n",
    "    if y > 0:\n",
    "        return y\n",
    "    else:\n",
    "        return 0.\n",
    "eta_over_s = np.vectorize(eta_over_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tt = np.linspace(0.1, 0.4, 100)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10,8),\n",
    "                         sharex=False, sharey=False, constrained_layout=True)\n",
    "fig.suptitle(\"Specefic Shear Viscosity posterior for uniform prior\", wrap=True)\n",
    "\n",
    "#n_samples_prior = 100000\n",
    "prune = 1\n",
    "prior_etas = []\n",
    "    \n",
    "for row in samples_df.iloc[0:100000,[7,8,9,10]].values:\n",
    "    [T_k, alow, ahigh, etas_k] = row\n",
    "    prior=[]\n",
    "    for T in Tt:\n",
    "        prior.append(eta_over_s(T,T_k,alow,ahigh,etas_k))\n",
    "    prior_etas.append(prior)\n",
    "per0,per5,per20,per80,per95,per100=np.percentile(prior_etas,[0,5,20,80,95,100], axis=0)\n",
    "axes.fill_between(Tt, per0,per100,color=sns.color_palette()[9], alpha=0.1, label='100% C.I.')\n",
    "axes.fill_between(Tt,per5,per95,color=sns.color_palette()[9], alpha=0.2, label='90% C.I.')\n",
    "axes.fill_between(Tt,per20,per80, color=sns.color_palette()[9], alpha=0.3, label='60% C.I.')\n",
    "pos=np.array(prior_etas).T\n",
    "#axes.violinplot(pos[1::10,:].T, positions=Tt[1::10],widths=0.03)\n",
    "\n",
    "axes.legend(loc='upper left')\n",
    "#axes.set_ylim(0,1.2)\n",
    "axes.set_xlabel('T [GeV]')\n",
    "axes.set_ylabel('$\\eta/s$')\n",
    "save_fig('shear_pos_uniform_prior')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tt = np.linspace(0.1, 0.4, 100)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10,8),\n",
    "                         sharex=False, sharey=False, constrained_layout=True)\n",
    "fig.suptitle(\"Specefic bulk Viscosity posterior for uniform prior\", wrap=True)\n",
    "\n",
    "prune = 1\n",
    "prior_zetas = []\n",
    "    \n",
    "for row in samples_df.iloc[0:100000,[11,12,13,14]].values:\n",
    "    [zmax, T0, width, asym] = row   \n",
    "    prior=[]\n",
    "    for T in Tt:\n",
    "        prior.append(zeta_over_s(T,zmax, T0, width, asym))\n",
    "    prior_zetas.append(prior)\n",
    "per0,per5,per20,per80,per95,per100=np.percentile(prior_zetas,[0,5,20,80,95,100], axis=0)\n",
    "axes.fill_between(Tt, per0,per100,color=sns.color_palette()[4], alpha=0.1, label='100% C.I.')\n",
    "axes.fill_between(Tt,per5,per95,color=sns.color_palette()[4], alpha=0.2, label='90% C.I.')\n",
    "axes.fill_between(Tt,per20,per80, color=sns.color_palette()[4], alpha=0.3, label='60% C.I.')\n",
    "pos=np.array(prior_zetas).T\n",
    "#axes.violinplot(pos[1::10,:].T, positions=Tt[1::10],widths=0.03)\n",
    "\n",
    "axes.legend(loc='upper right')\n",
    "#axes.set_ylim(0,1.2)\n",
    "axes.set_xlabel('T [GeV]')\n",
    "axes.set_ylabel('$\\zeta/s$')\n",
    "save_fig('bulk_pos_uniform_prior')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To do list\n",
    "> Emulator validation\n",
    "\n",
    "> Closure tests\n",
    "\n",
    "> Observable predictions from MAP\n",
    "\n",
    "> Including other systems in the parameter estimation\n",
    "\n",
    "> Tests for MCMC convergence\n",
    "\n",
    "> Bayesian evidence calculation\n",
    "\n",
    "> Repeat the Analysis for different model choices\n",
    "\n",
    "> Bayesian Model Averaging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
